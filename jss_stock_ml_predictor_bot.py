# -*- coding: utf-8 -*-
"""JSS_Stock_ML_Predictor_Bot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CYvu3xfv1k2wv753ELLO4X0mNOJpd4HM
"""

# Commented out IPython magic to ensure Python compatibility.
#Need to install finance package
!pip install yfinance
import yfinance as yf


#Need to define start and end dates to define scope of Stock values
from datetime import datetime
end = datetime.now()
start = datetime(end.year - 10, end.month, end.day) #can subtract 20 years from current year


#Initalize Stock variable
stock = "SQ"
blockinc_data = yf.download(stock, start, end)


#Visualize data from past 10 years
blockinc_data.head()       #table data
blockinc_data.shape        #shape of table (rows, columns)
blockinc_data.describe()   #data statisitics
blockinc_data.info()       #data types inside data set (null values, floats, etc)
blockinc_data.isna().sum() #checks for any null values and sums it up, if zero then data is clean set


#Graphing Data
import matplotlib.pyplot as plt
# %matplotlib inline

plt.figure(figsize = (15,5))
blockinc_data['Close'].plot() #using Ajd Close Price as it is more accurate than Close Price


#Creating Function to plot graphs for each stock input by user
def plot_graph(figsize, values, column_name):
  plt.figure(figsize = (15,5))
  values.plot(figsize = figsize)

  plt.xlabel("Years")
  plt.ylabel(column_name)
  plt.title(f"{column_name} of Block Inc Data")

blockinc_data.columns


#For loop to graph each with columns
for column in blockinc_data.columns:
  plot_graph((15,5), blockinc_data[column], column)


#Pandas Dataframe
##We will be inputting data to neural network to train and predict following day stock data##
import pandas as pd
data = pd.DataFrame(blockinc_data)
data.head()

for i in range(2015,2026):                          #counting number of days of each year stock data accounts for
  print(i, list(blockinc_data.index.year).count(i))


#Moving Average Method (MA) for 250 Days
blockinc_data['Moving Average (MA) for 250 days'] = blockinc_data['Close'].rolling(250).mean()
plot_graph((15,5), blockinc_data['Moving Average (MA) for 250 days'], 'MA_for_250 days')
plot_graph((15,5), blockinc_data[['Close', 'Moving Average (MA) for 250 days']], 'Moving Average (MA) for 250 days') #plots MA and Adj Close Price


#Moving Average Method (MA) for 100 Days
blockinc_data['Moving Average (MA) for 100 days'] = blockinc_data['Close'].rolling(100).mean()
plot_graph((15,5), blockinc_data[['Close', 'Moving Average (MA) for 100 days']], 'Moving Average (MA) for 100 days')


#Combined MA Plot
plot_graph((15,5), blockinc_data[['Close', 'Moving Average (MA) for 250 days', 'Moving Average (MA) for 100 days']], 'Combined MA') #the 100 days curve (green) is closer to the actual Adj Close data than the 250 days curve


#Percentage Change
blockinc_data['Percentage Change'] = blockinc_data['Close'].pct_change()
plot_graph((15,5), blockinc_data['Percentage Change'], 'Percentage Change')

Adj_Close_Price = blockinc_data['Close']
max(Adj_Close_Price.values),min(Adj_Close_Price.values) #shows max and min adj close price values in array form


#Converting data 0 to 1 range easier to train model rather than 8 - 281 (min and max values of stock)
from sklearn.preprocessing import MinMaxScaler #importing MinMax Scaler to scale down range
scaler = MinMaxScaler(feature_range = (0,1))   # 0 to 1 range --> easier for model to train
scaled_data = scaler.fit_transform(Adj_Close_Price)
scaled_data

len(scaled_data) #shows that the scaled data is same length as the original set from yfinance package


# MA Concept 100 (as our graph shows 100 is closer to true value rather than 250 day) rows to use as Input Training Data
## 1 to 100 to predict 101 row, and 2 to 101 to predict 102, and so forth till dataset ends at 2301
x_data = []
y_data = []

for i in range(100, len(scaled_data)): #need to start at row 100 or bc 99 or before wont have enough days for the 100 day MA
  x_data.append(scaled_data[i-100:i])
  y_data.append(scaled_data[i])        #predicting y values by using x data


#Numpy (converting x and y data into Numpy Arrays to make easier for analysis)
import numpy as np
x_data = np.array(x_data)
y_data = np.array(y_data)

x_data[0], y_data[0] #presents 100 rows of entrys for x array and predicts following y to be 0.02179637 at 101 row


#Splitting Data into Training and Testing Sets (70/30 split for training and testing split)
int(len(x_data)*0.7) # = 1540, 30% = 2301 - 100 - 1540 = 661

len_split = int(len(x_data)*0.7)

x_training = x_data[:len_split] #splitting array length by indexing
y_training = y_data[:len_split]

x_testing = x_data[len_split:]
y_testing = y_data[len_split:]

print(x_training.shape, y_training.shape)
print(x_testing.shape, y_testing.shape)


#Neural Network Long Short-Term Memory (LSTM) Model to Predict Closing Price using Keras (Keras is a high-level neural networks API that can run on top of TensorFlow)
from keras.models import Sequential
from keras.layers import Dense, Dropout, LSTM

model = Sequential()
model.add(LSTM(128, return_sequences = True, input_shape = (x_training.shape[1], 1))) #128 neurons, return seq values as true becuase input to the next layer is either a single vector or sequence of vectors and we are inputting seq of vectors
model.add(LSTM(64, return_sequences = False))                                         #second layer is also LSTM but 64 neurons, return seq value is false as not being returned as a sequence
model.add(Dense(25))                                                                  #25 neuron
model.add(Dense(1))                                                                   #1 neuron for the final layer

#Compiling Model
model.compile(optimizer = 'adam', loss = 'mean_squared_error') #optimizer Adam algorithm allows to optimize the running of the model, loss is matrix using Mean Squared Error

#Model Fitting (using training data to fit into model)
model.fit(x_training, y_training, batch_size = 1, epochs = 2) #batch size will segregate our input data that is training data into batches --> I am using default value of 1, epochs will train data set into second training session after alr being done in batch --> I am using 2, but don't use too many else too long and can cause overfitting

model.summary()

#Predicting Stock Data using Model using Test Data (not 70% Training Data)
stock_prediction = model.predict(x_testing) #is for x testing rn, will need to do y test as well

stock_prediction

#Inverse Transform Data to Find Values in terms of Original Stock Data
inverse_predictor = scaler.inverse_transform(stock_prediction)
inverse_predictor

#Inverse Transform y testing data
inverse_y_testing = scaler.inverse_transform(y_testing)
inverse_y_testing

#Error Calculations via Root Mean Square Error
RMSE = np.sqrt(np.mean(inverse_predictor - inverse_y_testing)**2)
RMSE # 2 is pretty low error

#Overall Graph Visualization
data_plot = pd.DataFrame(
 {
  'original_testing_data': inverse_y_testing.reshape(-1),
  'predicted_testing_data': inverse_predictor.reshape(-1)
 },
    index = blockinc_data.index[len_split+100:]
)

data_plot.head()

plot_graph((15,6), data_plot, 'Test Data')

plot_graph((15,6), pd.concat([Adj_Close_Price[:len_split+100], data_plot], axis=0), 'Whole Data')

#Need to save the model
model.save('New_Stock_Price_Prediction_Model.keras')