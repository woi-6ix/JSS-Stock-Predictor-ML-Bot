
#Moving Average Method (MA) for 100 Days
blockinc_data['Moving Average (MA) for 100 days'] = blockinc_data['Adj Close'].rolling(100).mean()
plot_graph((15,5), blockinc_data[['Adj Close', 'Moving Average (MA) for 100 days']], 'Moving Average (MA) for 100 days')


#Combined MA Plot
plot_graph((15,5), blockinc_data[['Adj Close', 'Moving Average (MA) for 250 days', 'Moving Average (MA) for 100 days']], 'Combined MA') #the 100 days curve (green) is closer to the actual Adj Close data than the 250 days curve


#Percentage Change
blockinc_data['Percentage Change'] = blockinc_data['Adj Close'].pct_change()
plot_graph((15,5), blockinc_data['Percentage Change'], 'Percentage Change')

Adj_Close_Price = blockinc_data['Adj Close']
max(Adj_Close_Price.values),min(Adj_Close_Price.values) #shows max and min adj close price values in array form


#Converting data 0 to 1 range easier to train model rather than 8 - 281 (min and max values of stock)
from sklearn.preprocessing import MinMaxScaler #importing MinMax Scaler to scale down range
scaler = MinMaxScaler(feature_range = (0,1))   # 0 to 1 range --> easier for model to train
scaled_data = scaler.fit_transform(Adj_Close_Price)
scaled_data

len(scaled_data) #shows that the scaled data is same length as the original set from yfinance package


# MA Concept 100 (as our graph shows 100 is closer to true value rather than 250 day) rows to use as Input Training Data
## 1 to 100 to predict 101 row, and 2 to 101 to predict 102, and so forth till dataset ends at 2301
x_data = []
y_data = []

for i in range(100, len(scaled_data)): #need to start at row 100 or bc 99 or before wont have enough days for the 100 day MA
  x_data.append(scaled_data[i-100:i])
  y_data.append(scaled_data[i])        #predicting y values by using x data


#Numpy (converting x and y data into Numpy Arrays to make easier for analysis)
import numpy as np
x_data = np.array(x_data)
y_data = np.array(y_data)

x_data[0], y_data[0] #presents 100 rows of entrys for x array and predicts following y to be 0.02179637 at 101 row


#Splitting Data into Training and Testing Sets (70/30 split for training and testing split)
int(len(x_data)*0.7) # = 1540, 30% = 2301 - 100 - 1540 = 661

len_split = int(len(x_data)*0.7)

x_training = x_data[:len_split] #splitting array length by indexing
y_training = y_data[:len_split]

x_testing = x_data[len_split:]
y_testing = y_data[len_split:]

print(x_training.shape, y_training.shape)
print(x_testing.shape, y_testing.shape)


#Neural Network Long Short-Term Memory (LSTM) Model to Predict Closing Price using Keras (Keras is a high-level neural networks API that can run on top of TensorFlow)
from keras.models import Sequential
from keras.layers import Dense, Dropout, LSTM

model = Sequential()
model.add(LSTM(128, return_sequences = True, input_shape = (x_training.shape[1], 1))) #128 neurons, return seq values as true becuase input to the next layer is either a single vector or sequence of vectors and we are inputting seq of vectors
model.add(LSTM(64, return_sequences = False))                                         #second layer is also LSTM but 64 neurons, return seq value is false as not being returned as a sequence
model.add(Dense(25))                                                                  #25 neuron
model.add(Dense(1))                                                                   #1 neuron for the final layer

#Compiling Model
model.compile(optimizer = 'adam', loss = 'mean_squared_error') #optimizer Adam algorithm allows to optimize the running of the model, loss is matrix using Mean Squared Error

#Model Fitting (using training data to fit into model)
model.fit(x_training, y_training, batch_size = 1, epochs = 2) #batch size will segregate our input data that is training data into batches --> I am using default value of 1, epochs will train data set into second training session after alr being done in batch --> I am using 2, but don't use too many else too long and can cause overfitting
